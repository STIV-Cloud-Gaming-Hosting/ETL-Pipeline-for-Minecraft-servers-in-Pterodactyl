{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pterodactyl Minecraft Logs\n",
    "\n",
    "### Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Setup directories, functions and folder creation\n",
    "- Extract logs from each active Minecraft Server\n",
    "- Transformation from logs data into information\n",
    "- Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile, requests, gzip, csv, os, re\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection\n",
    "host = os.environ['POSTGRES_HOST']\n",
    "port = os.environ['POSTGRES_PORT']\n",
    "database = os.environ['POSTGRES_DATABASE']\n",
    "username = os.environ['POSTGRES_USERNAME']\n",
    "password = os.environ['POSTGRES_PASSWORD']\n",
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.environ['PTERODACTYL_URL']\n",
    "application_api_key = os.environ['PTERODACTYL_APP_KEY']\n",
    "client_api_key = os.environ['PTERODACTYL_CLI_KEY']\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories, functions and folder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "raw_logs_folder = os.path.join(pwd, 'raw_logs')\n",
    "\n",
    "# Create new folder if not exists\n",
    "def mkdir(folder_dir):\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(os.path.join(pwd, folder_dir))\n",
    "\n",
    "# Sort a list of logs names\n",
    "def sort_list_logs(logs):\n",
    "    def logs_modifications(log):\n",
    "        # remove the '-' from the log\n",
    "        date_part, number_part = log.split('-')[:3], log.split('-')[3]\n",
    "        # Join the parts of date to transform\n",
    "        date_log = '-'.join(date_part)\n",
    "        # remove the '.log' extension and pass the number to int\n",
    "        number_log = int(number_part.split('.')[0])\n",
    "        # return the date and number for sorting\n",
    "        return date_log, number_log\n",
    "\n",
    "    # use the logs_modifications function to sort the logs by date and number\n",
    "    sorted_logs = sorted(logs, key=logs_modifications)\n",
    "    return sorted_logs\n",
    "\n",
    "# Get last index from a list when matching with an element\n",
    "def last_index(list, element):\n",
    "    try:\n",
    "        return len(list) - 1 - list[::-1].index(element)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"'{element}' not found in the list\")\n",
    "    \n",
    "# Extract the compressed files (.gz and .zip)    \n",
    "def extract_compressed_file(compressed_file_path, decompressed_folder_path):\n",
    "    if compressed_file_path.endswith('.gz'):\n",
    "        with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "            with open(decompressed_folder_path, 'wb') as decompressed_file:\n",
    "                decompressed_file.write(compressed_file.read())\n",
    "    elif compressed_file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(compressed_file_path, 'r') as zip_file:\n",
    "            zip_file.extractall(os.path.dirname(decompressed_folder_path))\n",
    "    \n",
    "# Create folder if not exist\n",
    "#mkdir(raw_logs_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract logs from each active Minecraft Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pattern = r'^\\d{4}-\\d{2}-\\d{2}-\\d.*' # yyyy-mm-dd-n*\n",
    "extension_file_compressed = '.log.gz'\n",
    "extension_file_uncompessed = '.log'\n",
    "SCHEMA_PTERODACTYL = 'pterodactyl'\n",
    "SCHEMA_MINECRAFT = 'minecraft'\n",
    "\n",
    "eggs_ready = ('Vanilla Minecraft', 'Forge Minecraft', 'Paper') # Vanilla Bedrock is still not ready to be processed\n",
    "\n",
    "# Get server information\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "    list_servers = conn.execute(text(f'SELECT servers.identifier, eggs.name, last_log_date.last_date  FROM {SCHEMA_PTERODACTYL}.servers JOIN {SCHEMA_PTERODACTYL}.eggs ON eggs.id = servers.egg_id JOIN {SCHEMA_MINECRAFT}.last_log_date ON last_log_date.server_identifier = servers.identifier WHERE servers.is_active = true and servers.nest_id = 1'))\n",
    "\n",
    "for server_info in list_servers:\n",
    "\n",
    "    if server_info[1] in eggs_ready:\n",
    "        # Create a folder\n",
    "        folder_name = server_info[0]\n",
    "        folder_server_dir = os.path.join(raw_logs_folder, folder_name)\n",
    "        mkdir(folder_server_dir)\n",
    "\n",
    "        # Try to get userchache.json file from server\n",
    "        try:\n",
    "            # Download users in cache\n",
    "            users_cache = api_cli.client.servers.files.get_file_contents(server_info[0], 'usercache.json')\n",
    "            user_names = [user['name'] for user in users_cache]\n",
    "        except:\n",
    "            user_names = [None]\n",
    "\n",
    "        # Add new users only\n",
    "        with open(os.path.join(raw_logs_folder, folder_name, 'users.csv'), 'a+', newline='') as csvfile:\n",
    "            csvfile.seek(0)\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            existing_names = [row['name'] for row in reader]\n",
    "            new_names = [name for name in user_names if name not in existing_names]\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['name'])\n",
    "            if not existing_names:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for name in new_names:\n",
    "                writer.writerow({'name': name})\n",
    "\n",
    "        # Get the last log date from postgres\n",
    "        last_log_date = server_info[2].strftime('%Y-%m-%d')\n",
    "\n",
    "        # Get a sorted list of the logs inside the server\n",
    "        log_files = api_cli.client.servers.files.list_files(server_info[0], '/logs/')\n",
    "        list_logs = [file['attributes']['name'] for file in log_files['data'] if re.match(log_pattern, file['attributes']['name'])]\n",
    "        sorted_list_logs = sort_list_logs(list_logs)\n",
    "        sorted_list_logs_date = [item[:10] for item in sorted_list_logs]\n",
    "        # Select only a list of downloadable_logs which are new; after the last_log_date\n",
    "        try:\n",
    "            index_last_log = last_index(sorted_list_logs_date, last_log_date)\n",
    "        except:\n",
    "            index_last_log = -1\n",
    "        downloadable_logs = sorted_list_logs[index_last_log + 1:]\n",
    "        # Download all logs in the list downloadable_logs\n",
    "        list_download = [api_cli.client.servers.files.download_file(server_info[0], f'/logs/{log}') for log in downloadable_logs]\n",
    "        if list_download:\n",
    "            [urllib.request.urlretrieve(list_download[i], os.path.join(folder_server_dir, list_logs[i])) for i in range(len(list_download))]\n",
    "        print(f'Files downloaded: {len(list_download)}')\n",
    "\n",
    "        # Uncompressing files\n",
    "        for filename in os.listdir(folder_server_dir):\n",
    "            if filename.endswith(extension_file_compressed):\n",
    "                compressed_file_path = os.path.join(folder_server_dir,filename)\n",
    "                decompressed_file_path = os.path.splitext(compressed_file_path)[0] # Remove the .gz extension\n",
    "\n",
    "                # Uncompress the file\n",
    "                with gzip.open(compressed_file_path, 'rb') as compressed_file:\n",
    "                    with open(decompressed_file_path, 'wb') as decompressed_file:\n",
    "                        decompressed_file.write(compressed_file.read())\n",
    "\n",
    "                # Delete the compressed file\n",
    "                os.remove(compressed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation from logs data to information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_logs = pd.DataFrame(columns=['server_identifier', 'date', 'time', 'information', 'user', 'activity'])\n",
    "\n",
    "for folder in os.listdir(raw_logs_folder): \n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "\n",
    "    # Read all logs as one\n",
    "    log_files = [log for log in os.listdir(folder_server_dir) if log.endswith(extension_file_uncompessed)]\n",
    "    log_files = sort_list_logs(log_files)\n",
    "\n",
    "    if log_files:\n",
    "\n",
    "        all_logs = \"\"\n",
    "        for log_file in log_files:\n",
    "            with open(os.path.join(folder_server_dir, log_file), 'r', encoding='utf-8') as file:\n",
    "                log_contents = file.read().split('\\n')\n",
    "                # This wont work with 2 digits (get the date from the file name)\n",
    "                log_contents = \"\\n\".join([f'[{log_file[:-(2+len(extension_file_uncompessed))]}] ' + line for line in log_contents if line.strip() != \"\"])\n",
    "                all_logs += log_contents + \"\\n\"\n",
    "\n",
    "        # Transform information it in meaningful information\n",
    "        pattern = r'\\[(\\d{4}-\\d{2}-\\d{2})\\] \\[.*?(\\d{2}:\\d{2}:\\d{2}).*?\\] \\[(.*?)/.*?\\]: (.*?)\\n'\n",
    "        matches = re.findall(pattern, all_logs)\n",
    "\n",
    "        # Create a list of dictionaries to store the extracted data\n",
    "        log_data = [{'server_identifier': folder, 'date': match[0], 'time': match[1], 'category': match[2], 'information': match[3]} for match in matches]\n",
    "        # Create a dataframe of the logs\n",
    "        df_logs = pd.DataFrame(log_data)\n",
    "        df_logs['user'] = None\n",
    "        df_logs['activity'] = None\n",
    "\n",
    "        # Filter by column category selecting only Server thread logs nad delete that column\n",
    "        df_logs = df_logs[df_logs['category'] == 'Server thread'][['server_identifier', 'date', 'time', 'information', 'user', 'activity']]\n",
    "\n",
    "        # Create a dataframe of the users\n",
    "        df_users = pd.read_csv(os.path.join(folder_server_dir,'users.csv'))\n",
    "\n",
    "        # Add information when server started\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Starting minecraft server version\") | df_logs['information'].str.startswith(\"Starting minecraft server on\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'start'\n",
    "        # Add information when server stopped\n",
    "        index=df_logs[df_logs['information'].str.startswith(\"Stopping server\") | df_logs['information'].str.startswith(\"Stopping the server\")].index\n",
    "        if not index.empty:\n",
    "            df_logs.loc[index, 'user'] = 'server'\n",
    "            df_logs.loc[index, 'activity'] = 'stop'\n",
    "        # Add information when user did something\n",
    "        for user in df_users['name']:\n",
    "            # Add information when user activity\n",
    "            index=df_logs[df_logs['information'].str.startswith(user)].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'action'\n",
    "            # Add information when user login\n",
    "            index=df_logs[df_logs['information'].str.startswith(f'{user} joined the game')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'login'\n",
    "            # Add information when user logout\n",
    "            index = df_logs[df_logs['information'].str.startswith(f'{user} left the game')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = user\n",
    "                df_logs.loc[index, 'activity'] = 'logout'\n",
    "            # Delete duplicated activity of login and logout\n",
    "            index=df_logs[df_logs['information'].str.startswith(f'{user}[') | df_logs['information'].str.startswith(f'{user} lost connection: Disconnected')].index\n",
    "            if not index.empty:\n",
    "                df_logs.loc[index, 'user'] = None\n",
    "                df_logs.loc[index, 'activity'] = None\n",
    "        # Rows with no server/user activity is deleted\n",
    "        df_logs = df_logs.dropna(subset=['activity'])\n",
    "\n",
    "        # Save every log in df_all_logs for each iteration\n",
    "        df_all_logs = pd.concat([df_all_logs, df_logs], ignore_index=True)\n",
    "\n",
    "# Delete all logs inside each server folder\n",
    "for folder in os.listdir(raw_logs_folder):\n",
    "    folder_server_dir = os.path.join(raw_logs_folder, folder)\n",
    "    [os.remove(os.path.join(folder_server_dir, log)) for log in os.listdir(folder_server_dir) if log.endswith(extension_file_uncompessed)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed data into Data Warehouse (Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and upload all new logs into table\n",
    "engine = create_engine(connection)\n",
    "with engine.connect() as conn:\n",
    "\n",
    "# Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Load all new activity into postgres\n",
    "        df_all_logs.to_sql(name = 'activity', schema = SCHEMA_MINECRAFT, con = conn, if_exists='append', index=False)\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
