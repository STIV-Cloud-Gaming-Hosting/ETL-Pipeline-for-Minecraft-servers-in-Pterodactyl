{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Pterodactyl Application\n",
    "\n",
    "### Index\n",
    "\n",
    "- Install requierements\n",
    "- Import libraries and setup key variables\n",
    "- Setup directories, functions and folder creation\n",
    "- Get Pterodactyl Application information\n",
    "- Upload csv table files into Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup key variables\n",
    "Remember to add you own credentials in the .env file for them to be loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, csv, os\n",
    "from sqlalchemy import create_engine, text\n",
    "from pydactyl import PterodactylClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file credentials\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection\n",
    "host = os.getenv('POSTGRES_HOST')\n",
    "port = os.getenv('POSTGRES_PORT')\n",
    "database = os.getenv('POSTGRES_DATABASE')\n",
    "username = os.getenv('POSTGRES_USERNAME')\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "connection = f'postgresql://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Pterodactyl connection\n",
    "pterodactyl_url = os.getenv('PTERODACTYL_URL')\n",
    "application_api_key = os.getenv('PTERODACTYL_APP_KEY')\n",
    "client_api_key = os.getenv('PTERODACTYL_CLI_KEY')\n",
    "\n",
    "# Connecto to Pterodactyl Application API\n",
    "api_app = PterodactylClient(pterodactyl_url, application_api_key, debug=False)\n",
    "# Connecto to Pterodactyl Client API\n",
    "api_cli = PterodactylClient(pterodactyl_url, client_api_key, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories, functions and folder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "pwd = os.getcwd() #os.path.dirname(os.path.realpath(__file__)) this is used for .py files\n",
    "server_app_folder = os.path.join(pwd, 'server_app_data')\n",
    "\n",
    "# Function definition\n",
    "from functions import save_to_csv, sort_list_logs, flatten_list\n",
    "\n",
    "# Create new folder if not exists\n",
    "def mkdir(folder_dir):\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(os.path.join(pwd, folder_dir))\n",
    "\n",
    "# Create folder if not exist\n",
    "mkdir(server_app_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pterodactyl Application information\n",
    "About: locations, nodes, nests, eggs, servers, clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data from Pterodactyl App\n",
    "list_of_clients = api_app.user.list_users()\n",
    "all_clients = [[client['attributes'] for client in clients]for clients in list_of_clients]\n",
    "\n",
    "list_of_locations = api_app.locations.list_locations()\n",
    "all_locations = [[location['attributes'] for location in locations]for locations in list_of_locations]\n",
    "\n",
    "list_of_nodes = api_app.nodes.list_nodes()\n",
    "all_nodes = [[node['attributes']for node in nodes] for nodes in list_of_nodes]\n",
    "\n",
    "list_of_nodes_and_allocations = api_app.nodes.list_nodes(includes=['allocations'])\n",
    "all_allocations = [[[{'node_id': node['attributes']['id'], **allocations['attributes']} for allocations in node['attributes']['relationships']['allocations']['data']] for node in nodes] for nodes in list_of_nodes_and_allocations][0]\n",
    "\n",
    "list_of_nests_and_eggs = api_app.nests.list_nests(includes=['eggs'])\n",
    "all_nests = [[nest['attributes'] for nest in nests] for nests in list_of_nests_and_eggs]\n",
    "all_eggs = [[eggs['attributes'] for eggs in nests['attributes']['relationships']['eggs']['data']] for nests in list_of_nests_and_eggs]\n",
    "\n",
    "list_of_servers_and_clients = api_app.servers.list_servers(includes=['subusers'])\n",
    "all_servers = [[server['attributes'] for server in servers] for servers in list_of_servers_and_clients]\n",
    "all_client_server = [[client_server['attributes'] for client_server in servers['attributes']['relationships']['subusers']['data']] for servers in list_of_servers_and_clients]\n",
    "\n",
    "# Get the current timestamp with timezone information (UTC)\n",
    "last_update = datetime.datetime.now(datetime.timezone.utc)\n",
    "\n",
    "# Cleaning and filtering columns\n",
    "df_clients = pd.DataFrame(all_clients[0])\n",
    "df_clients = df_clients[['id', 'uuid', 'username', 'email', 'first_name', 'last_name', 'root_admin', '2fa', 'created_at', 'updated_at']].rename(columns={'username': 'client_name', 'root_admin': 'admin'})\n",
    "\n",
    "df_locations = pd.DataFrame(all_locations[0])\n",
    "df_locations = df_locations[['id', 'short', 'long', 'created_at', 'updated_at']]\n",
    "\n",
    "df_nodes = pd.DataFrame(all_nodes[0])\n",
    "df_nodes['allocated_memory'] = df_nodes['allocated_resources'].apply(lambda x: x.get('memory', None))\n",
    "df_nodes['allocated_disk'] = df_nodes['allocated_resources'].apply(lambda x: x.get('disk', None))\n",
    "df_nodes = df_nodes[['id', 'uuid', 'public', 'name', 'description', 'location_id', 'fqdn', 'scheme', 'behind_proxy', 'maintenance_mode', 'memory', 'disk', 'allocated_memory', 'allocated_disk', 'upload_size', 'daemon_listen', 'daemon_sftp', 'daemon_base','created_at', 'updated_at']].rename(columns={'': '', '': ''})\n",
    "\n",
    "df_allocations = pd.DataFrame(flatten_list(all_allocations))\n",
    "df_allocations = df_allocations[['id', 'port', 'assigned', 'node_id']]\n",
    "\n",
    "df_nests = pd.DataFrame(all_nests[0])\n",
    "df_nests = df_nests[['id', 'uuid', 'name', 'description', 'author', 'created_at', 'updated_at']]\n",
    "\n",
    "df_eggs = pd.DataFrame(all_eggs[0])\n",
    "df_eggs = df_eggs[['id', 'uuid', 'name', 'description', 'nest', 'author', 'created_at', 'updated_at']].rename(columns={'nest': 'nest_id'})\n",
    "\n",
    "df_servers = pd.DataFrame(all_servers[0])\n",
    "df_servers['limit_memory'] = df_servers['limits'].apply(lambda x: x.get('memory', None))\n",
    "df_servers['limit_disk'] = df_servers['limits'].apply(lambda x: x.get('disk', None))\n",
    "df_servers['limit_io'] = df_servers['limits'].apply(lambda x: x.get('io', None))\n",
    "df_servers['limit_cpu'] = df_servers['limits'].apply(lambda x: x.get('cpu', None))\n",
    "df_servers['limit_oom_disable'] = df_servers['limits'].apply(lambda x: x.get('oom_disable', None))\n",
    "df_servers['limit_database'] = df_servers['feature_limits'].apply(lambda x: x.get('database', None))\n",
    "df_servers['limit_allocation'] = df_servers['feature_limits'].apply(lambda x: x.get('allocation', None))\n",
    "df_servers['limit_backup'] = df_servers['feature_limits'].apply(lambda x: x.get('backup', None))\n",
    "df_servers = df_servers[['id', 'uuid', 'identifier', 'name', 'description', 'limit_memory', 'limit_disk', 'limit_io', 'limit_cpu', 'limit_oom_disable', 'limit_database', 'limit_allocation', 'limit_backup', 'user', 'node', 'allocation', 'nest', 'egg','created_at', 'updated_at']].rename(columns={'user': 'client_id', 'node': 'node_id', 'allocation': 'allocation_id', 'nest': 'nest_id', 'egg': 'egg_id'})\n",
    "\n",
    "flattened_client_server = [item for sublist in all_client_server for item in sublist]\n",
    "df_clients_server = pd.DataFrame(flattened_client_server)[['id', 'user_id', 'server_id', 'created_at', 'updated_at']].rename(columns={'user_id': 'client_id'})\n",
    "df_clients_server\n",
    "\n",
    "# Exporting data into .csv files\n",
    "df_clients.to_csv(os.path.join(server_app_folder, 'clients.csv'), index=False)\n",
    "df_locations.to_csv(os.path.join(server_app_folder,'locations.csv'), index=False)\n",
    "df_nodes.to_csv(os.path.join(server_app_folder,'nodes.csv'), index=False)\n",
    "df_allocations.to_csv(os.path.join(server_app_folder,'allocations.csv'), index=False)\n",
    "df_nests.to_csv(os.path.join(server_app_folder,'nests.csv'), index=False)\n",
    "df_eggs.to_csv(os.path.join(server_app_folder,'eggs.csv'), index=False)\n",
    "df_servers.to_csv(os.path.join(server_app_folder,'servers.csv'), index=False)\n",
    "df_clients_server.to_csv(os.path.join(server_app_folder,'clients_server.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload csv table files into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database variables\n",
    "ID = 'id'\n",
    "TABLE = file_table.split('.')[0]\n",
    "TABLE_UPDATE = TABLE + '_update'\n",
    "IS_ACTIVE_TABLE = 'is_active_table'\n",
    "SCHEMA = 'pterodactyl'\n",
    "SCHEMA_UPDATE = 'pterodactyl_update'\n",
    "\n",
    "engine = create_engine(connection)\n",
    "\n",
    "for file_table in os.listdir(server_app_folder):\n",
    "    \n",
    "    # Reading of the file_table\n",
    "    df = pd.read_csv(os.path.join(server_app_folder, file_table))\n",
    "\n",
    "    # Start connection with database\n",
    "    with engine.connect() as conn:\n",
    "        # Start a new transaction\n",
    "        trans = conn.begin()\n",
    "\n",
    "        try:\n",
    "            # Load ID from database\n",
    "            result = conn.execute(text(f'SELECT \"{ID}\" FROM {SCHEMA}.{TABLE}'))\n",
    "            db = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "            # Compare ID\n",
    "            sameID = db[ID].isin(df[ID])\n",
    "\n",
    "            toUpdate = df[df[ID].isin(db[ID][sameID])]\n",
    "            toIngest = df[~df[ID].isin(db[ID][sameID])]\n",
    "            toDelete = db[~db[ID].isin(df[ID])]\n",
    "\n",
    "            # Insert the DataFrame into a table\n",
    "            toIngest.to_sql(TABLE, conn, schema=SCHEMA, if_exists='append', index=False)\n",
    "\n",
    "            # Insert the updatable DataFrame into the TABLE_UPDATE table\n",
    "            toUpdate.to_sql(TABLE_UPDATE, conn, schema=SCHEMA_UPDATE, if_exists='append', index=False)\n",
    "\n",
    "            # Define and execute the following queries\n",
    "            conn.execute(text(f'DELETE FROM {SCHEMA}.{TABLE} WHERE \"{ID}\" IN (SELECT \"{ID}\" FROM {SCHEMA_UPDATE}.{TABLE_UPDATE});'))\n",
    "            conn.execute(text(f'INSERT INTO {SCHEMA}.{TABLE} SELECT * FROM {SCHEMA_UPDATE}.{TABLE_UPDATE};'))\n",
    "            conn.execute(text(f'TRUNCATE TABLE {SCHEMA_UPDATE}.{TABLE_UPDATE};'))\n",
    "\n",
    "            # Update column \"is_active\" from tables when data is deleted from Pterodactyl App\n",
    "            toDelete.to_sql(IS_ACTIVE_TABLE, conn, schema=SCHEMA_UPDATE, if_exists='append', index=False)\n",
    "            conn.execute(text(f'UPDATE {SCHEMA}.{TABLE} SET is_active = false WHERE \"{ID}\" IN (SELECT * FROM {SCHEMA_UPDATE}.{IS_ACTIVE_TABLE});'))\n",
    "            conn.execute(text(f'TRUNCATE TABLE {SCHEMA_UPDATE}.{IS_ACTIVE_TABLE};'))\n",
    "\n",
    "            # Commit the transaction\n",
    "            trans.commit()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Rollback the transaction on exception\n",
    "            print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "            trans.rollback()\n",
    "            print('Transaction has been rolled back')\n",
    "            print(f'Error occurred during transaction:\\n{e}')\n",
    "            raise\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Start a new transaction\n",
    "    trans = conn.begin()\n",
    "\n",
    "    try:\n",
    "        # Update date from the last_update table based on max date on the file\n",
    "        old_last_update = conn.execute(text(f'SELECT date FROM {SCHEMA}.last_update')).fetchall()[0][0]\n",
    "        new_last_update = last_update\n",
    "        if new_last_update > old_last_update:\n",
    "            conn.execute(text(f\"UPDATE {SCHEMA}.last_update SET date = '{new_last_update}';\"))\n",
    "\n",
    "        # Commit the transaction\n",
    "        trans.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Rollback the transaction on exception\n",
    "        print('!!! [ERROR IN DATABASE QUERIES] !!!')\n",
    "        trans.rollback()\n",
    "        print('Transaction has been rolled back')\n",
    "        print(f'Error occurred during transaction:\\n{e}')\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
